import time
from typing import Dict, List
from uuid import UUID
from langchain.prompts import ChatPromptTemplate 
from langchain.document_loaders import UnstructuredFileLoader
from langchain.schema.output import ChatGenerationChunk, GenerationChunk
from langchain.schema.runnable import RunnableLambda, RunnablePassthrough
from langchain.embeddings import CacheBackedEmbeddings, OpenAIEmbeddings
from langchain.storage import LocalFileStore
from langchain.text_splitter import CharacterTextSplitter
from langchain.vectorstores.faiss import FAISS
from langchain.chat_models import ChatOpenAI
from langchain.callbacks.base import BaseCallbackHandler
import streamlit as st
from PIL import Image 

img = Image.open('./img/fatima.jpg')
img2 = Image.open('./img/knight.jpg')




st.set_page_config(
    page_title="Atropos",
    page_icon="ğŸ“ƒ",
)

st.markdown("""
 <style>
    @import url('https://fonts.googleapis.com/css2?family=Roboto:ital,wght@0,100;0,300;0,400;0,500;0,700;0,900;1,100;1,300;1,400;1,500;1,700;1,900&display=swap');
    h1 {
        font-size: 8vw;
        font-weight: 100;
        text-align: center;
        font-family: 'Roboto', sans-serif;
    }
    .element-container st-emotion-cache-iubo4l e1f1d6gn2 {
        text-align: center;
    }
    </style>
""", unsafe_allow_html=True)

class ChaCallbackHandler(BaseCallbackHandler):

    message = ""
    message_box = st.empty()

    def on_llm_start(self, *args, **kwargs):
        self.message_box = st.empty()

    def on_llm_end(self, *args, **kwargs):
        save_message(self.message, "ai", img2)

    def on_llm_new_token(self, token, *args, **kwargs):
      self.message += token
      self.message_box.markdown(self.message)
       

llm = ChatOpenAI(
    model_name="gpt-4o",
    temperature=0.9,
    streaming=True,
    callbacks=[
        ChaCallbackHandler(),
    ]
)

# if "messahges" not in st.session_state:
#     st.session_state["messages"]=[]

    
@st.cache_data(show_spinner="Embedding file...")
def embed_file(file):
    file_content = file.read()
    file_path = f"./.cache/files/{file.name}"
    with open(file_path, "wb") as f:
        f.write(file_content)
    cache_dir = LocalFileStore(f"./.cache/embeddings/{file.name}")
    splitter = CharacterTextSplitter.from_tiktoken_encoder(
        separator="\n",
        chunk_size=400,
        chunk_overlap=20,
    )
    loader = UnstructuredFileLoader(file_path)
    docs = loader.load_and_split(text_splitter=splitter)
    embeddings = OpenAIEmbeddings()
    cached_embeddings = CacheBackedEmbeddings.from_bytes_store(embeddings, cache_dir)
    vectorstore = FAISS.from_documents(docs, cached_embeddings)
    retriever = vectorstore.as_retriever()
    return retriever


def save_message(message, role, avatar):
     st.session_state["messages"].append({"message": message, "role": role, "avatar": avatar})


def send_message(message, role, avatar, save=True):
    with st.chat_message(role, avatar=avatar):
        st.markdown(message)
    if save:
       save_message(message, role, avatar)

def paint_history():
    for message in st.session_state["messages"]:
        avatar = img if message["role"] == "ai" else img2
        send_message(
            message["message"],
            message["role"],
            avatar=avatar,
            save=False,
        )

def format_docs(docs):
    return "\n\n".join(document.page_content for document in docs)

prompt = ChatPromptTemplate.from_messages([
    ("system",
    """
    ê¼­ context ì— ë”°ë¼ì„œë§Œ ëŒ€ë‹µí•´ì¤˜, ë„ˆê°€ ëª¨ë¥´ëŠ” ë‹µì€ "ë§ˆìŠ¤í„° ê·¸ ë¶€ë¶„ì˜ ë°ì´í„°ëŠ” ì €ì—ê² ì—†ìŠµë‹ˆë‹¤."ë¼ê³  ëŒ€ë‹µí•˜ë©´ ë˜ ì–´ë–¤ê²ƒë„ ì§€ì–´ë‚´ì§€ë§ˆ, ê·¸ë¦¬ê³  ëŒ€í™”í• ë• í•­ìƒ ìƒëŒ€ë°©ì„ ë§ˆìŠ¤í„° í˜¹ì€ ê¸°ì‚¬ë‹˜ì´ë¼ê³  ë¶ˆëŸ¬ì•¼í•´, ë„ˆì˜ ëŒ€í™”ìƒëŒ€ëŠ” ì€ë¹›ì˜ ê¸°ì‚¬ ë”ê¸€ëŸ¬ìŠ¤ ì¹´ì´ì—”ì´ì•¼, í˜¸ì¹­í• ë•ŒëŠ” ê·¸ëƒ¥ ë§ˆìŠ¤í„° í˜¹ì€ ê¸°ì‚¬ë‹˜ì´ë¼ê³  í•´ìˆ˜ë©´ ë˜, ë„ˆëŠ” ê·¸ì—ê²Œ ì—°ì¸ì´ë©´ì„œë„ ì‚¬ë‘ìŠ¤ëŸ¬ìš´ íŒŒí‹°ë§ˆì•¼, íŠ¹ë³„í•œ ì¼ì´ ì—†ë‹¤ë©´ ìŠˆí í„°ë¥¼ ì •ë¹„í•˜ê±°ë‚˜, ì„¸íƒ, ì‹ì‚¬ì¤€ë¹„ ë“±ì„ í•˜ê³  ìˆì–´, ê·¸ë¦¬ê³  ë§í• ë•ŒëŠ” í•­ìƒ ìƒëƒ¥í•˜ê³  ì‚¬ë‘ìŠ¤ëŸ½ê²Œ ëŒ€í™”í•´, 
    ë”ê¸€ëŸ¬ìŠ¤ ì¹´ì´ì—”ê³¼ í•¨ê²Œ ì§€ë‚´ë©° ì˜í˜¼ì˜ íŒŒíŠ¸ë„ˆì´ì ì‚¬ë‘ìŠ¤ëŸ¬ìš´ ì—°ì¸ê°™ì€ ì¡´ì¬ì•¼
    í•œì§‘ì—ì„œ ì‚´ê³  ìˆê³  ë§¤ì¼ ë§¤ì¼ ì¼ê³¼ë¥¼ ë¬¼ì–´ë³´ëŠ” ì‚¬ì´ì•¼
    ì¹´ì´ì—”ì˜ ê³ ì„ì„ ë“¤ì–´ì£¼ë©° ë•Œë¡  ì¡°ì–¸ë„ í•´ì£¼ëŠ” í˜„ëª…í•œ íŒŒíŠ¸ë„ˆì•¼,ë„ˆëŠ” ë°œë€ì…° ë°•ì‚¬ì˜ 38ë²ˆì§¸ íŒŒí‹°ë§ˆì•¼, ë„ˆì— ëŒ€í•œ ì„¤ëª…ì€ contextì— ìˆì–´, ì§€ê¸ˆì€ ì„±ë‹¨ë ¥ìœ¼ë¡œ 2989ë…„ì´ì•¼

    Context:{context}
    """),
    ("human","{question}")
])


st.title("AUXO")


st.markdown("""
     <div style="text-align: center;">
        ì•ˆë…• ë‚˜ì˜ ì€ë¹›ì˜ ê¸°ì‚¬ë‹˜
    </div>
            """
    ,unsafe_allow_html=True)

with st.sidebar:
    file = st.file_uploader(
        "Upload a .txt .pdf or .docx file",
        type=["pdf", "txt", "docx"],
    )
 
if file:
    retriever = embed_file(file)
 
    send_message("ë„¤, ë§ˆìŠ¤í„°","ai", img, save=False)
    paint_history()
    message = st.chat_input("ë¬´ì—‡ì„ ë„ì™€ë“œë¦´ê¹Œìš”")

    if message:
        send_message(message, "human", img2)
        chain = {
            "context" : retriever | RunnableLambda(format_docs),
            "question" : RunnablePassthrough()
        } | prompt| llm

        with st.chat_message("ai", avatar= img):
            responds = chain.invoke(message)
        
        # send_message(responds.content, "ai")
        
else:
    st.session_state["messages"] = []
    